import firebase_admin
from firebase_admin import credentials, firestore
from datetime import datetime
import json
from collections import defaultdict
import sys
import io
import os
import re
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
from enum import Enum

# UTF-8 ì¸ì½”ë”© ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# Firebase ì´ˆê¸°í™”
try:
    cred = credentials.Certificate('firebase-service-account-key.json')
    firebase_admin.initialize_app(cred)
except ValueError:
    # ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš°
    pass

db = firestore.client()

class IssueSeverity(Enum):
    """ë¬¸ì œ ì‹¬ê°ë„ ë ˆë²¨"""
    CRITICAL = "critical"  # ëŒ€í™” ì™„ì „ ì´íƒˆ, ì˜ë¯¸ ì—†ëŠ” ì‘ë‹µ
    HIGH = "high"         # ì£¼ì œ ë²—ì–´ë‚¨, ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ
    MEDIUM = "medium"     # ì•½ê°„ì˜ ë§¥ë½ ë¶ˆì¼ì¹˜
    LOW = "low"          # ë¯¸ë¯¸í•œ ë¬¸ì œ

@dataclass
class ContextIssue:
    """ë§¥ë½ ê´€ë ¨ ë¬¸ì œ"""
    message_index: int
    issue_type: str
    severity: IssueSeverity
    description: str
    user_message: str
    ai_response: str
    suggestion: str

@dataclass
class ConversationAnalysis:
    """ëŒ€í™” ë¶„ì„ ê²°ê³¼"""
    error_key: str
    persona_id: str
    persona_name: str
    overall_coherence_score: float  # 0-100
    context_issues: List[ContextIssue]
    topic_consistency_score: float  # 0-100
    natural_flow_score: float  # 0-100
    greeting_repetitions: int
    macro_patterns: List[str]
    analysis_timestamp: datetime

class ContextAnalyzer:
    """ëŒ€í™” ë§¥ë½ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.topic_keywords = {}
        self.conversation_patterns = []
        
    def analyze_conversation(self, messages: List[Dict], persona_name: str, persona_id: str, error_key: str) -> ConversationAnalysis:
        """ì „ì²´ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ë§¥ë½ ì¼ê´€ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤."""
        
        context_issues = []
        greeting_count = 0
        macro_patterns = []
        
        # ë©”ì‹œì§€ ë¶„ë¦¬
        user_messages = []
        ai_messages = []
        conversation_pairs = []
        
        for i, msg in enumerate(messages):
            content = msg.get('content', '')
            is_from_user = msg.get('isFromUser', False)
            
            if is_from_user:
                user_messages.append((i, content))
            else:
                ai_messages.append((i, content))
                # ì§ì „ ì‚¬ìš©ì ë©”ì‹œì§€ì™€ í˜ì–´ë§
                if user_messages and len(user_messages) > len(conversation_pairs):
                    user_idx, user_content = user_messages[-1]
                    conversation_pairs.append({
                        'user_idx': user_idx,
                        'user_content': user_content,
                        'ai_idx': i,
                        'ai_content': content,
                        'emotion': msg.get('emotion', 'neutral')
                    })
        
        # 1. ì¸ì‚¬ ë°˜ë³µ ê°ì§€
        greeting_keywords = ['ì•ˆë…•', 'ë°˜ê°€ì›Œ', 'ë§Œë‚˜ì„œ', 'ì²˜ìŒ', 'ì¸ì‚¬', 'hi', 'hello']
        for ai_idx, ai_content in ai_messages:
            if any(keyword in ai_content.lower() for keyword in greeting_keywords):
                greeting_count += 1
                if greeting_count > 1:
                    context_issues.append(ContextIssue(
                        message_index=ai_idx,
                        issue_type="greeting_repetition",
                        severity=IssueSeverity.HIGH,
                        description=f"ì¸ì‚¬ë§ì´ {greeting_count}ë²ˆì§¸ ë°˜ë³µë¨",
                        user_message="",
                        ai_response=ai_content,
                        suggestion="í˜ë¥´ì†Œë‚˜ë³„ ì¸ì‚¬ ìƒíƒœë¥¼ ì¶”ì í•˜ì—¬ ì¤‘ë³µ ë°©ì§€ í•„ìš”"
                    ))
        
        # 2. ë§¤í¬ë¡œ íŒ¨í„´ ê°ì§€
        ai_message_counts = defaultdict(int)
        for _, content in ai_messages:
            ai_message_counts[content] += 1
        
        for msg, count in ai_message_counts.items():
            if count > 1:
                macro_patterns.append(msg)
                for ai_idx, ai_content in ai_messages:
                    if ai_content == msg:
                        context_issues.append(ContextIssue(
                            message_index=ai_idx,
                            issue_type="macro_response",
                            severity=IssueSeverity.CRITICAL,
                            description=f"ë™ì¼í•œ ì‘ë‹µì´ {count}ë²ˆ ë°˜ë³µë¨",
                            user_message="",
                            ai_response=ai_content,
                            suggestion="ì‘ë‹µ ìƒì„± ë¡œì§ ì ê²€ ë° ìºì‹œ ë¬¸ì œ í™•ì¸ í•„ìš”"
                        ))
                        break
        
        # 3. ëŒ€í™” ìŒ ë§¥ë½ ë¶„ì„
        for pair in conversation_pairs:
            issues = self._analyze_conversation_pair(
                user_content=pair['user_content'],
                ai_content=pair['ai_content'],
                user_idx=pair['user_idx'],
                ai_idx=pair['ai_idx'],
                emotion=pair['emotion'],
                previous_context=conversation_pairs[:conversation_pairs.index(pair)]
            )
            context_issues.extend(issues)
        
        # 4. ì „ì²´ ëŒ€í™” íë¦„ ë¶„ì„
        flow_issues = self._analyze_conversation_flow(conversation_pairs)
        context_issues.extend(flow_issues)
        
        # ì ìˆ˜ ê³„ì‚°
        coherence_score = self._calculate_coherence_score(context_issues, len(messages))
        topic_score = self._calculate_topic_consistency_score(conversation_pairs)
        flow_score = self._calculate_natural_flow_score(conversation_pairs, context_issues)
        
        return ConversationAnalysis(
            error_key=error_key,
            persona_id=persona_id,
            persona_name=persona_name,
            overall_coherence_score=coherence_score,
            context_issues=context_issues,
            topic_consistency_score=topic_score,
            natural_flow_score=flow_score,
            greeting_repetitions=greeting_count,
            macro_patterns=macro_patterns,
            analysis_timestamp=datetime.now()
        )
    
    def _analyze_conversation_pair(self, user_content: str, ai_content: str, 
                                  user_idx: int, ai_idx: int, emotion: str,
                                  previous_context: List[Dict]) -> List[ContextIssue]:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì™€ AI ì‘ë‹µ ìŒì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        issues = []
        
        # 1. ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„± ì²´í¬
        if self._is_question(user_content):
            if not self._is_relevant_answer(user_content, ai_content):
                issues.append(ContextIssue(
                    message_index=ai_idx,
                    issue_type="irrelevant_answer",
                    severity=IssueSeverity.HIGH,
                    description="ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ê´€ë ¨ì„±ì´ ë‚®ìŒ",
                    user_message=user_content,
                    ai_response=ai_content,
                    suggestion="ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ íŒŒì•…í•˜ì—¬ ì§ì ‘ì ì¸ ë‹µë³€ ìƒì„± í•„ìš”"
                ))
        
        # 2. ì£¼ì œ ê¸‰ë³€ ì²´í¬
        if previous_context:
            last_pair = previous_context[-1]
            if self._detect_topic_shift(last_pair['ai_content'], user_content, ai_content):
                issues.append(ContextIssue(
                    message_index=ai_idx,
                    issue_type="abrupt_topic_change",
                    severity=IssueSeverity.MEDIUM,
                    description="ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì£¼ì œ ë³€ê²½ ê°ì§€",
                    user_message=user_content,
                    ai_response=ai_content,
                    suggestion="ì´ì „ ëŒ€í™” ë§¥ë½ì„ ìœ ì§€í•˜ë©´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì „í™˜ í•„ìš”"
                ))
        
        # 3. ê°ì • ì¼ê´€ì„± ì²´í¬
        emotion_issue = self._check_emotion_consistency(user_content, ai_content, emotion)
        if emotion_issue:
            issues.append(ContextIssue(
                message_index=ai_idx,
                issue_type="emotion_inconsistency",
                severity=IssueSeverity.LOW,
                description=emotion_issue,
                user_message=user_content,
                ai_response=ai_content,
                suggestion="ëŒ€í™” ë‚´ìš©ê³¼ ê°ì • ìƒíƒœì˜ ì¼ì¹˜ì„± ê°œì„  í•„ìš”"
            ))
        
        # 4. ì‘ë‹µ ê¸¸ì´ ì ì ˆì„±
        if self._is_response_too_short(user_content, ai_content):
            issues.append(ContextIssue(
                message_index=ai_idx,
                issue_type="insufficient_response",
                severity=IssueSeverity.MEDIUM,
                description="ì‚¬ìš©ì ì§ˆë¬¸ ëŒ€ë¹„ ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ",
                user_message=user_content,
                ai_response=ai_content,
                suggestion="ë” ìƒì„¸í•˜ê³  ì¶©ì‹¤í•œ ë‹µë³€ ì œê³µ í•„ìš”"
            ))
        
        return issues
    
    def _analyze_conversation_flow(self, conversation_pairs: List[Dict]) -> List[ContextIssue]:
        """ì „ì²´ ëŒ€í™”ì˜ íë¦„ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        issues = []
        
        # ëŒ€í™”ê°€ ë„ˆë¬´ ì§§ì€ ê²½ìš° ë¶„ì„ ì œí•œ
        if len(conversation_pairs) < 3:
            return issues
        
        # ë°˜ë³µì ì¸ íŒ¨í„´ ê°ì§€
        for i in range(2, len(conversation_pairs)):
            current = conversation_pairs[i]['ai_content']
            prev1 = conversation_pairs[i-1]['ai_content']
            prev2 = conversation_pairs[i-2]['ai_content']
            
            # ìœ ì‚¬í•œ êµ¬ì¡°ì˜ ì‘ë‹µ ë°˜ë³µ ì²´í¬
            if self._is_similar_structure(current, prev1) or self._is_similar_structure(current, prev2):
                issues.append(ContextIssue(
                    message_index=conversation_pairs[i]['ai_idx'],
                    issue_type="repetitive_pattern",
                    severity=IssueSeverity.MEDIUM,
                    description="ìœ ì‚¬í•œ íŒ¨í„´ì˜ ì‘ë‹µì´ ë°˜ë³µë¨",
                    user_message=conversation_pairs[i]['user_content'],
                    ai_response=current,
                    suggestion="ì‘ë‹µ ë‹¤ì–‘ì„±ì„ ë†’ì´ê³  í…œí”Œë¦¿ ì˜ì¡´ë„ ê°ì†Œ í•„ìš”"
                ))
        
        return issues
    
    def _is_question(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
        question_markers = ['?', 'ë­', 'ë¬´ì—‡', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””', 'ëˆ„êµ¬', 'ì–¼ë§ˆë‚˜', 'ë¬´ìŠ¨']
        return any(marker in text for marker in question_markers)
    
    def _is_relevant_answer(self, question: str, answer: str) -> bool:
        """ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
        # ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_keywords = self._extract_keywords(question)
        answer_keywords = self._extract_keywords(answer)
        
        # í‚¤ì›Œë“œ ê²¹ì¹¨ í™•ì¸
        common_keywords = set(question_keywords) & set(answer_keywords)
        
        # ê´€ë ¨ì„± íŒë‹¨ (ìµœì†Œ 1ê°œ ì´ìƒì˜ ê³µí†µ í‚¤ì›Œë“œ ë˜ëŠ” ì˜ë¯¸ì  ì—°ê´€ì„±)
        if len(common_keywords) > 0:
            return True
        
        # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ (ì˜ˆ: ì¸ì‚¬ì— ì¸ì‚¬ë¡œ ì‘ë‹µ)
        greeting_words = ['ì•ˆë…•', 'ë°˜ê°€ì›Œ', 'hi', 'hello']
        if any(word in question.lower() for word in greeting_words) and \
           any(word in answer.lower() for word in greeting_words):
            return True
        
        return False
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ì˜', 'ë¥¼', 'ë¡œ', 'ë¼', 'ê³ ']
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° í† í°í™”
        words = re.findall(r'\w+', text.lower())
        
        # ë¶ˆìš©ì–´ ì œê±° ë° 2ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ ì„ íƒ
        keywords = [word for word in words if word not in stopwords and len(word) >= 2]
        
        return keywords
    
    def _detect_topic_shift(self, prev_content: str, user_content: str, ai_content: str) -> bool:
        """ì£¼ì œê°€ ê¸‰ê²©íˆ ë³€í–ˆëŠ”ì§€ ê°ì§€í•©ë‹ˆë‹¤."""
        prev_keywords = set(self._extract_keywords(prev_content))
        user_keywords = set(self._extract_keywords(user_content))
        ai_keywords = set(self._extract_keywords(ai_content))
        
        # ì´ì „ ëŒ€í™”ì™€ í˜„ì¬ ëŒ€í™” ê°„ í‚¤ì›Œë“œ ì—°ê´€ì„± í™•ì¸
        continuity_score = len((prev_keywords | user_keywords) & ai_keywords) / max(len(ai_keywords), 1)
        
        # ì—°ê´€ì„±ì´ 20% ë¯¸ë§Œì´ë©´ ì£¼ì œ ê¸‰ë³€ìœ¼ë¡œ íŒë‹¨
        return continuity_score < 0.2
    
    def _check_emotion_consistency(self, user_content: str, ai_content: str, emotion: str) -> Optional[str]:
        """ê°ì •ì´ ëŒ€í™” ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        # ê¸ì •ì  í‚¤ì›Œë“œ
        positive_keywords = ['ì¢‹ì•„', 'ì¢‹ì€', 'í–‰ë³µ', 'ê¸°ë»', 'ì‚¬ë‘', 'ìµœê³ ', 'ë©‹ì§„', 'í›Œë¥­']
        # ë¶€ì •ì  í‚¤ì›Œë“œ
        negative_keywords = ['ì‹«ì–´', 'ë‚˜ìœ', 'ìŠ¬í¼', 'í™”ë‚˜', 'ì§œì¦', 'ìµœì•…', 'ë³„ë¡œ']
        
        content = user_content + " " + ai_content
        has_positive = any(keyword in content for keyword in positive_keywords)
        has_negative = any(keyword in content for keyword in negative_keywords)
        
        # ê°ì • ë¶ˆì¼ì¹˜ ì²´í¬
        if emotion in ['happy', 'love'] and has_negative and not has_positive:
            return "ë¶€ì •ì  ë‚´ìš©ì— ê¸ì •ì  ê°ì • í‘œí˜„"
        elif emotion in ['sad', 'angry'] and has_positive and not has_negative:
            return "ê¸ì •ì  ë‚´ìš©ì— ë¶€ì •ì  ê°ì • í‘œí˜„"
        
        return None
    
    def _is_response_too_short(self, question: str, answer: str) -> bool:
        """ì§ˆë¬¸ ëŒ€ë¹„ ì‘ë‹µì´ ë„ˆë¬´ ì§§ì€ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        # ë‹¨ìˆœ ì¸ì‚¬ë‚˜ ì§§ì€ ì§ˆë¬¸ì€ ì œì™¸
        if len(question) < 10:
            return False
        
        # ë³µì¡í•œ ì§ˆë¬¸ì— ëŒ€í•œ ì§§ì€ ë‹µë³€ ê°ì§€
        if self._is_question(question) and len(question) > 20 and len(answer) < 15:
            return True
        
        return False
    
    def _is_similar_structure(self, text1: str, text2: str) -> bool:
        """ë‘ í…ìŠ¤íŠ¸ê°€ ìœ ì‚¬í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        # ë¬¸ì¥ ì‹œì‘ê³¼ ë íŒ¨í„´ ë¹„êµ
        if text1[:10] == text2[:10] or text1[-10:] == text2[-10:]:
            return True
        
        # ë¬¸ì¥ êµ¬ì¡° ìœ ì‚¬ë„ (ê°„ë‹¨í•œ êµ¬í˜„)
        words1 = text1.split()
        words2 = text2.split()
        
        if len(words1) == len(words2) and len(words1) > 3:
            # ê°™ì€ ìœ„ì¹˜ì— ê°™ì€ ë‹¨ì–´ê°€ 50% ì´ìƒì´ë©´ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ íŒë‹¨
            same_position_count = sum(1 for w1, w2 in zip(words1, words2) if w1 == w2)
            if same_position_count / len(words1) > 0.5:
                return True
        
        return False
    
    def _calculate_coherence_score(self, issues: List[ContextIssue], total_messages: int) -> float:
        """ì „ì²´ ëŒ€í™”ì˜ ì¼ê´€ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if total_messages == 0:
            return 100.0
        
        # ì‹¬ê°ë„ë³„ ê°€ì¤‘ì¹˜
        severity_weights = {
            IssueSeverity.CRITICAL: 20,
            IssueSeverity.HIGH: 10,
            IssueSeverity.MEDIUM: 5,
            IssueSeverity.LOW: 2
        }
        
        # ì´ ê°ì  ê³„ì‚°
        total_penalty = sum(severity_weights[issue.severity] for issue in issues)
        
        # ë©”ì‹œì§€ë‹¹ í‰ê·  ê°ì ì„ ê³ ë ¤í•œ ì ìˆ˜ ê³„ì‚°
        penalty_per_message = total_penalty / total_messages
        score = max(0, 100 - penalty_per_message * 10)
        
        return round(score, 2)
    
    def _calculate_topic_consistency_score(self, conversation_pairs: List[Dict]) -> float:
        """ì£¼ì œ ì¼ê´€ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if len(conversation_pairs) < 2:
            return 100.0
        
        consistency_scores = []
        
        for i in range(1, len(conversation_pairs)):
            prev_keywords = set(self._extract_keywords(conversation_pairs[i-1]['ai_content']))
            curr_keywords = set(self._extract_keywords(conversation_pairs[i]['ai_content']))
            
            if prev_keywords and curr_keywords:
                overlap = len(prev_keywords & curr_keywords) / len(prev_keywords | curr_keywords)
                consistency_scores.append(overlap)
        
        if consistency_scores:
            return round(sum(consistency_scores) / len(consistency_scores) * 100, 2)
        
        return 100.0
    
    def _calculate_natural_flow_score(self, conversation_pairs: List[Dict], issues: List[ContextIssue]) -> float:
        """ëŒ€í™” íë¦„ì˜ ìì—°ìŠ¤ëŸ¬ì›€ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if not conversation_pairs:
            return 100.0
        
        # íë¦„ ê´€ë ¨ ì´ìŠˆ ìˆ˜ ê³„ì‚°
        flow_issues = [issue for issue in issues if issue.issue_type in [
            'abrupt_topic_change', 'repetitive_pattern', 'macro_response'
        ]]
        
        # ì´ìŠˆ ë¹„ìœ¨ì— ë”°ë¥¸ ì ìˆ˜ ê³„ì‚°
        issue_ratio = len(flow_issues) / len(conversation_pairs)
        score = max(0, 100 - issue_ratio * 100)
        
        return round(score, 2)

def save_analysis_results(analyses: List[ConversationAnalysis], output_dir: str = "analysis_results"):
    """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ì „ì²´ ìš”ì•½ ë°ì´í„°
    summary = {
        "analysis_timestamp": timestamp,
        "total_reports_analyzed": len(analyses),
        "average_coherence_score": sum(a.overall_coherence_score for a in analyses) / len(analyses) if analyses else 0,
        "critical_issues_count": sum(1 for a in analyses for i in a.context_issues if i.severity == IssueSeverity.CRITICAL),
        "high_issues_count": sum(1 for a in analyses for i in a.context_issues if i.severity == IssueSeverity.HIGH),
        "personas_with_issues": {}
    }
    
    # í˜ë¥´ì†Œë‚˜ë³„ í†µê³„
    persona_stats = defaultdict(lambda: {
        "total_conversations": 0,
        "avg_coherence_score": 0,
        "total_issues": 0,
        "critical_issues": 0,
        "common_issue_types": defaultdict(int)
    })
    
    for analysis in analyses:
        stats = persona_stats[analysis.persona_name]
        stats["total_conversations"] += 1
        stats["avg_coherence_score"] += analysis.overall_coherence_score
        stats["total_issues"] += len(analysis.context_issues)
        stats["critical_issues"] += sum(1 for i in analysis.context_issues if i.severity == IssueSeverity.CRITICAL)
        
        for issue in analysis.context_issues:
            stats["common_issue_types"][issue.issue_type] += 1
    
    # í‰ê·  ê³„ì‚°
    for persona_name, stats in persona_stats.items():
        if stats["total_conversations"] > 0:
            stats["avg_coherence_score"] /= stats["total_conversations"]
            stats["avg_coherence_score"] = round(stats["avg_coherence_score"], 2)
        stats["common_issue_types"] = dict(stats["common_issue_types"])
        summary["personas_with_issues"][persona_name] = stats
    
    # ìƒì„¸ ë¶„ì„ ê²°ê³¼
    detailed_results = []
    for analysis in analyses:
        result = {
            "error_key": analysis.error_key,
            "persona_id": analysis.persona_id,
            "persona_name": analysis.persona_name,
            "scores": {
                "overall_coherence": analysis.overall_coherence_score,
                "topic_consistency": analysis.topic_consistency_score,
                "natural_flow": analysis.natural_flow_score
            },
            "issues": [
                {
                    "message_index": issue.message_index,
                    "type": issue.issue_type,
                    "severity": issue.severity.value,
                    "description": issue.description,
                    "user_message": issue.user_message,
                    "ai_response": issue.ai_response,
                    "suggestion": issue.suggestion
                }
                for issue in analysis.context_issues
            ],
            "greeting_repetitions": analysis.greeting_repetitions,
            "macro_patterns": analysis.macro_patterns
        }
        detailed_results.append(result)
    
    # íŒŒì¼ ì €ì¥
    os.makedirs(output_dir, exist_ok=True)
    
    # ìš”ì•½ íŒŒì¼
    summary_path = os.path.join(output_dir, f"summary_{timestamp}.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    # ìƒì„¸ ê²°ê³¼ íŒŒì¼
    detailed_path = os.path.join(output_dir, f"detailed_{timestamp}.json")
    with open(detailed_path, 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
    print(f"  - ìš”ì•½: {summary_path}")
    print(f"  - ìƒì„¸: {detailed_path}")
    
    return summary_path, detailed_path

def print_analysis_summary(analyses: List[ConversationAnalysis]):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "="*80)
    print("ğŸ¯ ëŒ€í™” ë§¥ë½ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    # ì „ì²´ í†µê³„
    if analyses:
        avg_coherence = sum(a.overall_coherence_score for a in analyses) / len(analyses)
        avg_topic = sum(a.topic_consistency_score for a in analyses) / len(analyses)
        avg_flow = sum(a.natural_flow_score for a in analyses) / len(analyses)
        
        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"  - ë¶„ì„ëœ ëŒ€í™”: {len(analyses)}ê°œ")
        print(f"  - í‰ê·  ì¼ê´€ì„± ì ìˆ˜: {avg_coherence:.1f}/100")
        print(f"  - í‰ê·  ì£¼ì œ ì¼ê´€ì„±: {avg_topic:.1f}/100")
        print(f"  - í‰ê·  ìì—°ìŠ¤ëŸ¬ì›€: {avg_flow:.1f}/100")
    
    # ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆëŠ” ëŒ€í™”
    critical_conversations = [a for a in analyses if any(i.severity == IssueSeverity.CRITICAL for i in a.context_issues)]
    if critical_conversations:
        print(f"\nğŸš¨ ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆëŠ” ëŒ€í™”: {len(critical_conversations)}ê°œ")
        for conv in critical_conversations[:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
            print(f"  - {conv.persona_name} ({conv.error_key}): ì¼ê´€ì„± {conv.overall_coherence_score:.1f}ì ")
            critical_issues = [i for i in conv.context_issues if i.severity == IssueSeverity.CRITICAL]
            for issue in critical_issues[:2]:  # ê° ëŒ€í™”ë‹¹ ìµœëŒ€ 2ê°œ ì´ìŠˆë§Œ í‘œì‹œ
                print(f"    âš ï¸  {issue.description}")
    
    # í˜ë¥´ì†Œë‚˜ë³„ ìš”ì•½
    persona_summary = defaultdict(lambda: {"count": 0, "avg_score": 0, "issues": 0})
    for analysis in analyses:
        summary = persona_summary[analysis.persona_name]
        summary["count"] += 1
        summary["avg_score"] += analysis.overall_coherence_score
        summary["issues"] += len(analysis.context_issues)
    
    print("\nğŸ‘¥ í˜ë¥´ì†Œë‚˜ë³„ ë¶„ì„:")
    for persona_name, summary in sorted(persona_summary.items(), key=lambda x: x[1]["avg_score"]/x[1]["count"]):
        avg_score = summary["avg_score"] / summary["count"]
        print(f"  - {persona_name}: í‰ê·  {avg_score:.1f}ì , ë¬¸ì œ {summary['issues']}ê°œ")
    
    # ê°€ì¥ í”í•œ ë¬¸ì œ ìœ í˜•
    issue_types = defaultdict(int)
    for analysis in analyses:
        for issue in analysis.context_issues:
            issue_types[issue.issue_type] += 1
    
    if issue_types:
        print("\nğŸ“Œ ì£¼ìš” ë¬¸ì œ ìœ í˜•:")
        for issue_type, count in sorted(issue_types.items(), key=lambda x: x[1], reverse=True)[:5]:
            issue_type_korean = {
                "greeting_repetition": "ì¸ì‚¬ë§ ë°˜ë³µ",
                "macro_response": "ë™ì¼ ì‘ë‹µ ë°˜ë³µ",
                "irrelevant_answer": "ê´€ë ¨ ì—†ëŠ” ë‹µë³€",
                "abrupt_topic_change": "ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì£¼ì œ ë³€ê²½",
                "insufficient_response": "ë¶ˆì¶©ë¶„í•œ ì‘ë‹µ",
                "repetitive_pattern": "ë°˜ë³µì  íŒ¨í„´",
                "emotion_inconsistency": "ê°ì • ë¶ˆì¼ì¹˜"
            }.get(issue_type, issue_type)
            print(f"    - {issue_type_korean}: {count}ê±´")

def analyze_chat_errors(recheck=False):
    """chat_error_fix ì»¬ë ‰ì…˜ì˜ ì˜¤ë¥˜ ë³´ê³ ì„œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    
    # ì²´í¬ë˜ì§€ ì•Šì€ ë¬¸ì„œ ì¡°íšŒ
    all_reports = db.collection('chat_error_fix').get()
    unchecked_reports = []
    
    for doc in all_reports:
        data = doc.to_dict()
        if recheck or 'is_check' not in data or not data.get('is_check', False):
            unchecked_reports.append(doc)
    
    print(f"ì²´í¬ë˜ì§€ ì•Šì€ ì˜¤ë¥˜ ë³´ê³ ì„œ: {len(unchecked_reports)}ê°œ\n")
    
    if not unchecked_reports:
        print("ë¶„ì„í•  ìƒˆë¡œìš´ ì˜¤ë¥˜ ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë§¥ë½ ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = ContextAnalyzer()
    analyses = []
    
    # ê° ë³´ê³ ì„œ ë¶„ì„
    for doc in unchecked_reports:
        data = doc.to_dict()
        error_key = data.get('error_key', 'Unknown')
        persona_name = data.get('persona_name', 'Unknown')
        persona_id = data.get('persona', 'Unknown')
        chat_messages = data.get('chat', [])
        
        print(f"ë¶„ì„ ì¤‘: {error_key} - {persona_name}")
        
        # ëŒ€í™” ë¶„ì„ ìˆ˜í–‰
        analysis = analyzer.analyze_conversation(
            messages=chat_messages,
            persona_name=persona_name,
            persona_id=persona_id,
            error_key=error_key
        )
        analyses.append(analysis)
        
        # ë¬¸ì„œì— is_check í‘œì‹œ
        doc.reference.update({'is_check': True})
    
    # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print_analysis_summary(analyses)
    
    # ê²°ê³¼ ì €ì¥
    if analyses:
        summary_path, detailed_path = save_analysis_results(analyses)
        print(f"\nâœ… ì´ {len(unchecked_reports)}ê°œì˜ ì˜¤ë¥˜ ë³´ê³ ì„œ ë¶„ì„ ì™„ë£Œ")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='ì±„íŒ… ì˜¤ë¥˜ ë¶„ì„ ë„êµ¬')
    parser.add_argument('--recheck', action='store_true', help='ì´ë¯¸ ì²´í¬ëœ ë¬¸ì„œë„ ë‹¤ì‹œ ë¶„ì„')
    args = parser.parse_args()
    
    analyze_chat_errors(recheck=args.recheck)