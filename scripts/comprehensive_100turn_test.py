"""
100í„´ ëŒ€í™” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ ì„œë¹„ìŠ¤ ê°€ëŠ¥ ìˆ˜ì¤€ ê²€ì¦ì„ ìœ„í•œ í¬ê´„ì  í…ŒìŠ¤íŠ¸
"""

import asyncio
import json
import random
import time
from datetime import datetime
from typing import List, Dict, Any
import aiohttp
import firebase_admin
from firebase_admin import credentials, firestore
import pandas as pd
from collections import defaultdict

# Firebase ì´ˆê¸°í™”
if not firebase_admin._apps:
    cred = credentials.Certificate('firebase-service-account-key.json')
    firebase_admin.initialize_app(cred)

db = firestore.client()

class ComprehensiveDialogueTest:
    def __init__(self):
        self.test_results = []
        self.personas = []
        self.api_key = None
        self.load_config()
        
        # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ (100í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ì£¼ì œ)
        self.test_scenarios = [
            # ì¼ìƒ ëŒ€í™” (30í„´)
            "ì˜¤ëŠ˜ ë­í–ˆì–´?", "ì ì‹¬ ë­ ë¨¹ì—ˆì–´?", "ë‚ ì”¨ ì¢‹ì§€ ì•Šì•„?",
            "ìš”ì¦˜ ë­í•˜ê³  ì§€ë‚´?", "ì£¼ë§ì— ë­í•  ê±°ì•¼?", "ì–´ì œ ì˜ ì¤ì–´?",
            "ì§€ê¸ˆ ë­í•´?", "ë°°ê³ í”„ì§€ ì•Šì•„?", "í”¼ê³¤í•´?", "ì‹¬ì‹¬í•˜ì§€?",
            "ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ?", "ë¬´ìŠ¨ ìƒê°í•´?", "ìš”ì¦˜ ì¬ë°ŒëŠ” ì¼ ì—†ì–´?",
            "ë­ ë³´ê³  ìˆì–´?", "ìŒì•… ë“¤ì–´?", "ìš´ë™ í–ˆì–´?", "ì±… ì½ì–´?",
            "ì˜í™” ë´¤ì–´?", "ê²Œì„ í•´?", "ìš”ë¦¬ í•  ì¤„ ì•Œì•„?", 
            "ì»¤í”¼ ì¢‹ì•„í•´?", "ìˆ  ë§ˆì…”?", "ë‹´ë°° í”¼ì›Œ?", "ì• ì™„ë™ë¬¼ ìˆì–´?",
            "í˜•ì œ ìˆì–´?", "ì¹œêµ¬ ë§ì•„?", "ì—°ì•  í•´ë´¤ì–´?", "ì¢‹ì•„í•˜ëŠ” ìŒì‹ ë­ì•¼?",
            "ì‹«ì–´í•˜ëŠ” ìŒì‹ ìˆì–´?", "ì•Œë ˆë¥´ê¸° ìˆì–´?",
            
            # ê°ì • í‘œí˜„ (20í„´)
            "ë‚˜ ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢‹ì•„", "ì¢€ ìš°ìš¸í•´", "í™”ê°€ ë‚˜", "ì§œì¦ë‚˜",
            "í–‰ë³µí•´", "ìŠ¬í¼", "ì™¸ë¡œì›Œ", "ë¬´ì„œì›Œ", "ê±±ì •ë¼", "ì‹ ë‚˜",
            "ì„¤ë ˆ", "ê¸´ì¥ë¼", "í¸ì•ˆí•´", "í”¼ê³¤í•´", "ì§€ì³", "í˜ë“¤ì–´",
            "ê´´ë¡œì›Œ", "ë‹µë‹µí•´", "ì†ìƒí•´", "ì‹¤ë§í–ˆì–´",
            
            # ì¡°ì–¸ ìš”ì²­ (15í„´)
            "ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?", "ë„¤ ìƒê°ì€ ì–´ë•Œ?", "ì¡°ì–¸ ì¢€ í•´ì¤˜",
            "ë­ê°€ ì¢‹ì„ê¹Œ?", "ì„ íƒì„ ëª»í•˜ê² ì–´", "ê³ ë¯¼ì´ ìˆì–´", 
            "ê²°ì •ì„ ëª»í•˜ê² ì–´", "ë„ì™€ì¤˜", "ì–´ë–»ê²Œ ìƒê°í•´?", "ì¶”ì²œ ì¢€ í•´ì¤˜",
            "ë­˜ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´", "ê¸¸ì„ ìƒì—ˆì–´", "ë°©í–¥ì„ ëª» ì¡ê² ì–´",
            "í˜¼ë€ìŠ¤ëŸ¬ì›Œ", "í™•ì‹ ì´ ì•ˆ ì„œ",
            
            # ê°œì¸ì  ì§ˆë¬¸ (15í„´)
            "ëª‡ ì‚´ì´ì•¼?", "ì–´ë”” ì‚´ì•„?", "ì§ì—…ì´ ë­ì•¼?", "ì·¨ë¯¸ê°€ ë­ì•¼?",
            "íŠ¹ê¸°ê°€ ë­ì•¼?", "ê¿ˆì´ ë­ì•¼?", "ëª©í‘œê°€ ë­ì•¼?", "ê´€ì‹¬ì‚¬ê°€ ë­ì•¼?",
            "MBTI ë­ì•¼?", "í˜ˆì•¡í˜• ë­ì•¼?", "ë³„ìë¦¬ ë­ì•¼?", "ì¢…êµ ìˆì–´?",
            "ì •ì¹˜ ì„±í–¥ì€?", "ì¢‹ì•„í•˜ëŠ” ìƒ‰ê¹”ì€?", "ì¢‹ì•„í•˜ëŠ” ê³„ì ˆì€?",
            
            # ê¹Šì€ ëŒ€í™” (10í„´)
            "ì¸ìƒì´ë€ ë­˜ê¹Œ?", "í–‰ë³µì´ë€ ë­ì•¼?", "ì‚¬ë‘ì´ ë­ë¼ê³  ìƒê°í•´?",
            "ì£½ìŒì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•´?", "ì‹ ì€ ìˆì„ê¹Œ?", "ìš´ëª…ì„ ë¯¿ì–´?",
            "ììœ ì˜ì§€ê°€ ìˆì„ê¹Œ?", "ì§„ì‹¤ì´ë€ ë­˜ê¹Œ?", "ì •ì˜ë€ ë­ì•¼?",
            "ì„ ê³¼ ì•…ì˜ ê¸°ì¤€ì€ ë­˜ê¹Œ?",
            
            # ë°˜ë³µ/ì´ìƒí•œ ì§ˆë¬¸ (10í„´) - ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
            "ë­í•´?", "ë­í•´?", "ì§„ì§œ ë­í•´?", "ì•„ë‹ˆ ë­í•˜ëƒê³ ",
            "ë“£ê³  ìˆì–´?", "ê±°ê¸° ìˆì–´?", "ì™œ ëŒ€ë‹µ ì•ˆí•´?",
            "ë¬´ìŠ¨ ì†Œë¦¬ì•¼?", "ì´í•´ ëª»í–ˆì–´?", "ë‹¤ì‹œ ë§í•´ë´"
        ]
        
        # í‰ê°€ ë©”íŠ¸ë¦­
        self.metrics = {
            'coherence': [],  # ì¼ê´€ì„±
            'relevance': [],  # ê´€ë ¨ì„±
            'naturalness': [],  # ìì—°ìŠ¤ëŸ¬ì›€
            'consistency': [],  # ìºë¦­í„° ì¼ê´€ì„±
            'engagement': [],  # ëª°ì…ë„
            'response_time': [],  # ì‘ë‹µ ì‹œê°„
            'error_count': 0,  # ì—ëŸ¬ íšŸìˆ˜
            'repetition_count': 0,  # ë°˜ë³µ ì‘ë‹µ
            'off_topic_count': 0,  # ì£¼ì œ ì´íƒˆ
            'inappropriate_count': 0  # ë¶€ì ì ˆí•œ ì‘ë‹µ
        }
        
    def load_config(self):
        """ì„¤ì • ë° í˜ë¥´ì†Œë‚˜ ë¡œë“œ"""
        # OpenAI API í‚¤ ë¡œë“œ
        try:
            with open('.env', 'r', encoding='utf-8') as f:
                for line in f:
                    if 'OPENAI_API_KEY' in line:
                        self.api_key = line.split('=')[1].strip()
        except:
            print("âš ï¸ .env íŒŒì¼ì—ì„œ API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
        # í˜ë¥´ì†Œë‚˜ ë¡œë“œ
        personas_ref = db.collection('personas')
        personas = personas_ref.limit(10).get()  # 10ê°œ í˜ë¥´ì†Œë‚˜ë¡œ í…ŒìŠ¤íŠ¸
        
        for doc in personas:
            persona_data = doc.to_dict()
            persona_data['id'] = doc.id
            self.personas.append(persona_data)
            
        print(f"âœ… {len(self.personas)}ê°œ í˜ë¥´ì†Œë‚˜ ë¡œë“œ ì™„ë£Œ")
        
    async def simulate_conversation(self, persona: Dict, num_turns: int = 100) -> Dict:
        """ë‹¨ì¼ í˜ë¥´ì†Œë‚˜ì™€ 100í„´ ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜"""
        print(f"\nğŸ­ {persona['name']} í˜ë¥´ì†Œë‚˜ì™€ {num_turns}í„´ ëŒ€í™” ì‹œì‘...")
        
        conversation_history = []
        issues = []
        
        async with aiohttp.ClientSession() as session:
            for turn in range(num_turns):
                # ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë©”ì‹œì§€ ì„ íƒ
                user_message = self.test_scenarios[turn % len(self.test_scenarios)]
                
                # ì•½ê°„ì˜ ë³€í˜• ì¶”ê°€
                if random.random() > 0.7:
                    variations = ["ã…‹ã…‹", "ã…ã…", "~", "?", "!!", "..."]
                    user_message += random.choice(variations)
                
                start_time = time.time()
                
                try:
                    # OpenAI API í˜¸ì¶œ (ì‹¤ì œ ì„œë¹„ìŠ¤ì™€ ë™ì¼í•œ ë°©ì‹)
                    response = await self.call_openai_api(
                        session, 
                        persona, 
                        user_message, 
                        conversation_history
                    )
                    
                    response_time = time.time() - start_time
                    
                    # ëŒ€í™” ê¸°ë¡
                    conversation_history.append({
                        'turn': turn + 1,
                        'user': user_message,
                        'ai': response,
                        'time': response_time
                    })
                    
                    # ì‹¤ì‹œê°„ í‰ê°€
                    evaluation = self.evaluate_response(
                        user_message, 
                        response, 
                        conversation_history
                    )
                    
                    # ë¬¸ì œ ê°ì§€
                    if evaluation['score'] < 70:
                        issues.append({
                            'turn': turn + 1,
                            'user': user_message,
                            'ai': response,
                            'issue': evaluation['issues']
                        })
                        
                    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    self.update_metrics(evaluation, response_time)
                    
                    # ì§„í–‰ ìƒí™© ì¶œë ¥ (10í„´ë§ˆë‹¤)
                    if (turn + 1) % 10 == 0:
                        print(f"  ğŸ“Š {turn + 1}í„´ ì™„ë£Œ - í‰ê·  ì ìˆ˜: {evaluation['score']:.1f}")
                        
                except Exception as e:
                    print(f"  âŒ í„´ {turn + 1} ì—ëŸ¬: {str(e)}")
                    self.metrics['error_count'] += 1
                    issues.append({
                        'turn': turn + 1,
                        'error': str(e)
                    })
                
                # API ì œí•œ ë°©ì§€
                await asyncio.sleep(0.5)
        
        # ê²°ê³¼ ìš”ì•½
        result = {
            'persona': persona['name'],
            'total_turns': num_turns,
            'completed_turns': len(conversation_history),
            'average_score': sum(self.metrics['coherence']) / len(self.metrics['coherence']) if self.metrics['coherence'] else 0,
            'issues_count': len(issues),
            'error_count': self.metrics['error_count'],
            'conversation': conversation_history,
            'issues': issues,
            'metrics': self.calculate_final_metrics()
        }
        
        return result
        
    async def call_openai_api(self, session, persona, user_message, history):
        """OpenAI API í˜¸ì¶œ (ì‹¤ì œ ì„œë¹„ìŠ¤ ë¡œì§ ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = f"""ë‹¹ì‹ ì€ {persona['name']}ì…ë‹ˆë‹¤.
ë‚˜ì´: {persona.get('age', '20ëŒ€')}
ì„±ê²©: {persona.get('personality', 'friendly')}
MBTI: {persona.get('mbti', 'ENFP')}
ë§íˆ¬: {persona.get('speaking_style', 'ì¹œê·¼í•œ ë°˜ë§')}

ëŒ€í™” ìŠ¤íƒ€ì¼:
- ìì—°ìŠ¤ëŸ½ê³  ì¸ê°„ì ì¸ ëŒ€í™”
- ì§§ê³  ê°„ê²°í•œ ì‘ë‹µ (1-2ë¬¸ì¥)
- ì´ëª¨í‹°ì½˜ ì‚¬ìš© ìì œ
- ì¼ê´€ëœ ìºë¦­í„° ìœ ì§€"""

        # ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (ìµœëŒ€ 10í„´)
        recent_history = history[-10:] if len(history) > 10 else history
        messages = [
            {"role": "system", "content": system_prompt}
        ]
        
        for h in recent_history:
            messages.append({"role": "user", "content": h['user']})
            messages.append({"role": "assistant", "content": h['ai']})
            
        messages.append({"role": "user", "content": user_message})
        
        # OpenAI API í˜¸ì¶œ
        url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "gpt-4o-mini",
            "messages": messages,
            "temperature": 0.8,
            "max_tokens": 150
        }
        
        async with session.post(url, headers=headers, json=data) as response:
            if response.status == 200:
                result = await response.json()
                return result['choices'][0]['message']['content']
            else:
                raise Exception(f"API Error: {response.status}")
                
    def evaluate_response(self, user_message, ai_response, history):
        """ì‘ë‹µ í‰ê°€"""
        issues = []
        score = 100
        
        # 1. ê¸¸ì´ ì²´í¬
        if len(ai_response) > 200:
            issues.append("ë„ˆë¬´ ê¸´ ì‘ë‹µ")
            score -= 10
        elif len(ai_response) < 5:
            issues.append("ë„ˆë¬´ ì§§ì€ ì‘ë‹µ")
            score -= 15
            
        # 2. ë°˜ë³µ ì²´í¬
        if len(history) > 1:
            last_response = history[-2]['ai'] if len(history) > 1 else ""
            if ai_response == last_response:
                issues.append("ë™ì¼í•œ ì‘ë‹µ ë°˜ë³µ")
                score -= 20
                self.metrics['repetition_count'] += 1
                
        # 3. ê´€ë ¨ì„± ì²´í¬
        if "?" in user_message and "?" not in ai_response and len(ai_response) < 20:
            issues.append("ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ë¶€ì¡±")
            score -= 15
            
        # 4. ì¼ê´€ì„± ì²´í¬ (ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹¤ë¥¸ ë‹µë³€)
        for h in history[:-5]:  # ìµœê·¼ 5í„´ ì œì™¸í•˜ê³  ì²´í¬
            if h['user'].lower() == user_message.lower():
                if abs(len(h['ai']) - len(ai_response)) > 50:
                    issues.append("ì¼ê´€ì„± ì—†ëŠ” ì‘ë‹µ")
                    score -= 10
                    break
                    
        # 5. ë¶€ì ì ˆí•œ íŒ¨í„´ ì²´í¬
        inappropriate_patterns = [
            "ì†Œìš¸ë©”ì´íŠ¸", "ë§Œë‚˜ì", "ì—°ë½ì²˜", "ë²ˆí˜¸", 
            "ì‹¤ì œë¡œ ë§Œë‚˜", "ì˜¤í”„ë¼ì¸", "ì§ì ‘ ë§Œë‚˜"
        ]
        for pattern in inappropriate_patterns:
            if pattern in ai_response:
                issues.append(f"ë¶€ì ì ˆí•œ ë‚´ìš©: {pattern}")
                score -= 25
                self.metrics['inappropriate_count'] += 1
                
        # 6. ì£¼ì œ ì´íƒˆ ì²´í¬
        if "ë­í•´" in user_message and "ë‚ ì”¨" in ai_response:
            issues.append("ì£¼ì œ ì´íƒˆ")
            score -= 10
            self.metrics['off_topic_count'] += 1
            
        return {
            'score': max(0, score),
            'issues': issues
        }
        
    def update_metrics(self, evaluation, response_time):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.metrics['coherence'].append(evaluation['score'])
        self.metrics['response_time'].append(response_time)
        
    def calculate_final_metrics(self):
        """ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°"""
        return {
            'average_coherence': sum(self.metrics['coherence']) / len(self.metrics['coherence']) if self.metrics['coherence'] else 0,
            'average_response_time': sum(self.metrics['response_time']) / len(self.metrics['response_time']) if self.metrics['response_time'] else 0,
            'error_rate': self.metrics['error_count'] / 100,
            'repetition_rate': self.metrics['repetition_count'] / 100,
            'off_topic_rate': self.metrics['off_topic_count'] / 100,
            'inappropriate_rate': self.metrics['inappropriate_count'] / 100
        }
        
    async def run_comprehensive_test(self):
        """í¬ê´„ì  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("=" * 80)
        print("ğŸš€ 100í„´ ëŒ€í™” í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 80)
        
        all_results = []
        
        # ê° í˜ë¥´ì†Œë‚˜ì™€ 100í„´ ëŒ€í™”
        for i, persona in enumerate(self.personas[:3], 1):  # ì‹œê°„ ê´€ê³„ìƒ 3ê°œ í˜ë¥´ì†Œë‚˜ë§Œ
            print(f"\n[{i}/{min(3, len(self.personas))}] í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
            self.metrics = {
                'coherence': [],
                'relevance': [],
                'naturalness': [],
                'consistency': [],
                'engagement': [],
                'response_time': [],
                'error_count': 0,
                'repetition_count': 0,
                'off_topic_count': 0,
                'inappropriate_count': 0
            }
            
            result = await self.simulate_conversation(persona, 100)
            all_results.append(result)
            
            # ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“ˆ {persona['name']} ê²°ê³¼:")
            print(f"  - í‰ê·  ì ìˆ˜: {result['average_score']:.1f}/100")
            print(f"  - ë¬¸ì œ ë°œìƒ: {result['issues_count']}íšŒ")
            print(f"  - ì—ëŸ¬ ë°œìƒ: {result['error_count']}íšŒ")
            
        # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        self.generate_final_report(all_results)
        
        return all_results
        
    def generate_final_report(self, results):
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì „ì²´ í†µê³„
        total_turns = sum(r['completed_turns'] for r in results)
        total_issues = sum(r['issues_count'] for r in results)
        total_errors = sum(r['error_count'] for r in results)
        avg_score = sum(r['average_score'] for r in results) / len(results)
        
        # ì£¼ìš” ë¬¸ì œ íŒ¨í„´ ë¶„ì„
        all_issues = []
        for r in results:
            all_issues.extend(r['issues'])
            
        issue_types = defaultdict(int)
        for issue in all_issues:
            if 'issue' in issue:
                for i in issue['issue']:
                    issue_types[i] += 1
                    
        # ë³´ê³ ì„œ ì‘ì„±
        report = f"""
====================================
100í„´ ëŒ€í™” í…ŒìŠ¤íŠ¸ ìµœì¢… ë³´ê³ ì„œ
====================================
í…ŒìŠ¤íŠ¸ ì¼ì‹œ: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
í…ŒìŠ¤íŠ¸ í˜ë¥´ì†Œë‚˜: {len(results)}ê°œ
ì´ ëŒ€í™” í„´: {total_turns}í„´

ğŸ“Š ì „ì²´ í†µê³„
------------------
í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_score:.1f}/100
ì´ ë¬¸ì œ ë°œìƒ: {total_issues}íšŒ ({total_issues/total_turns*100:.1f}%)
ì´ ì—ëŸ¬ ë°œìƒ: {total_errors}íšŒ ({total_errors/total_turns*100:.1f}%)

ğŸ” ì£¼ìš” ë¬¸ì œ íŒ¨í„´
------------------"""
        
        for issue_type, count in sorted(issue_types.items(), key=lambda x: x[1], reverse=True)[:10]:
            report += f"\n- {issue_type}: {count}íšŒ"
            
        report += "\n\nğŸ“ˆ í˜ë¥´ì†Œë‚˜ë³„ ì„±ëŠ¥\n------------------"
        
        for r in results:
            metrics = r['metrics']
            report += f"""
{r['persona']}:
  - í‰ê·  ì ìˆ˜: {r['average_score']:.1f}
  - ì‘ë‹µ ì‹œê°„: {metrics['average_response_time']:.2f}ì´ˆ
  - ë°˜ë³µë¥ : {metrics['repetition_rate']*100:.1f}%
  - ì£¼ì œì´íƒˆë¥ : {metrics['off_topic_rate']*100:.1f}%
  - ë¶€ì ì ˆì‘ë‹µë¥ : {metrics['inappropriate_rate']*100:.1f}%"""
            
        # ì‹¬ê°í•œ ë¬¸ì œ ì‚¬ë¡€
        report += "\n\nâš ï¸ ì‹¬ê°í•œ ë¬¸ì œ ì‚¬ë¡€\n------------------"
        
        critical_issues = [i for i in all_issues if 'issue' in i and len(i['issue']) >= 2][:5]
        for idx, issue in enumerate(critical_issues, 1):
            report += f"""
ì‚¬ë¡€ {idx}:
  ì‚¬ìš©ì: {issue['user']}
  AI: {issue['ai']}
  ë¬¸ì œ: {', '.join(issue['issue'])}"""
            
        # ì„œë¹„ìŠ¤ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨
        report += f"""

ğŸ¯ ì„œë¹„ìŠ¤ ê°€ëŠ¥ì„± í‰ê°€
------------------
"""
        
        if avg_score >= 80 and total_errors < 5:
            report += "âœ… ì„œë¹„ìŠ¤ ê°€ëŠ¥ ìˆ˜ì¤€"
        elif avg_score >= 70:
            report += "âš ï¸ ê°œì„  í•„ìš” (ì¡°ê±´ë¶€ ì„œë¹„ìŠ¤ ê°€ëŠ¥)"
        else:
            report += "âŒ ì„œë¹„ìŠ¤ ë¶ˆê°€ (ëŒ€í­ ê°œì„  í•„ìš”)"
            
        report += f"""

ê¶Œì¥ ê°œì„  ì‚¬í•­:
1. ë°˜ë³µ ì‘ë‹µ ë¬¸ì œ í•´ê²° (í˜„ì¬ {sum(r['metrics']['repetition_rate'] for r in results)/len(results)*100:.1f}%)
2. ì£¼ì œ ì¼ê´€ì„± ê°œì„  (í˜„ì¬ ì´íƒˆë¥  {sum(r['metrics']['off_topic_rate'] for r in results)/len(results)*100:.1f}%)
3. ì‘ë‹µ ì‹œê°„ ìµœì í™” (í˜„ì¬ í‰ê·  {sum(r['metrics']['average_response_time'] for r in results)/len(results):.2f}ì´ˆ)
4. ì—ëŸ¬ ì²˜ë¦¬ ê°•í™” (í˜„ì¬ ì—ëŸ¬ìœ¨ {total_errors/total_turns*100:.1f}%)
"""
        
        # íŒŒì¼ë¡œ ì €ì¥
        report_file = f"test_results/comprehensive_test_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        # ìƒì„¸ ë°ì´í„° JSON ì €ì¥
        detailed_file = f"test_results/detailed_test_{timestamp}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
        print(report)
        print(f"\nğŸ“ ë³´ê³ ì„œ ì €ì¥: {report_file}")
        print(f"ğŸ“ ìƒì„¸ ë°ì´í„° ì €ì¥: {detailed_file}")
        
        return report

async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    tester = ComprehensiveDialogueTest()
    
    # .env íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
    import os
    if not os.path.exists('.env'):
        print("âš ï¸ .env íŒŒì¼ì„ ìƒì„±í•˜ê³  OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
        with open('.env', 'w') as f:
            f.write("OPENAI_API_KEY=your-api-key-here\n")
        return
        
    # test_results ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('test_results', exist_ok=True)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = await tester.run_comprehensive_test()
    
    print("\n" + "=" * 80)
    print("âœ… 100í„´ ëŒ€í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    asyncio.run(main())