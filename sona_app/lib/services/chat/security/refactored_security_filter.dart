import 'package:flutter/foundation.dart';
import '../../../models/persona.dart';
import '../security/system_info_protection.dart';
import '../security/pattern_detector_service.dart';

/// ğŸ”’ ë¦¬íŒ©í† ë§ëœ ë³´ì•ˆ í•„í„° ì„œë¹„ìŠ¤
/// 
/// íŒ¨í„´ ê°ì§€ë§Œ ìˆ˜í–‰í•˜ê³ , ì‹¤ì œ ì‘ë‹µì€ OpenAI APIì— ìœ„ì„
/// í•˜ë“œì½”ë”©ëœ ì‘ë‹µ ì™„ì „ ì œê±°
class RefactoredSecurityFilter {
  
  /// ğŸ›¡ï¸ ë³´ì•ˆ ë¶„ì„ (íŒ¨í„´ ì²´í¬ë§Œ)
  static SecurityAnalysisResult analyzeMessage({
    required String userMessage,
    List<String> recentMessages = const [],
  }) {
    // íŒ¨í„´ ê°ì§€ ì„œë¹„ìŠ¤ë¡œ ë¶„ì„
    final detection = PatternDetectorService.detectPatterns(userMessage);
    
    // ë¬¸ë§¥ ê¸°ë°˜ ì¶”ê°€ ìœ„í—˜ ë¶„ì„
    final contextualRisk = _analyzeContextualRisk(userMessage, recentMessages);
    
    // ìµœì¢… ìœ„í—˜ë„ ê³„ì‚°
    double finalRiskLevel = detection.riskLevel;
    if (contextualRisk) {
      finalRiskLevel = (finalRiskLevel + 0.3).clamp(0.0, 1.0);
    }
    
    return SecurityAnalysisResult(
      needsDeflection: detection.needsDeflection || contextualRisk,
      category: detection.category,
      riskLevel: finalRiskLevel,
      contextHint: _buildContextHint(detection, contextualRisk),
      isHighRisk: finalRiskLevel >= 0.8,
      isSafe: detection.isSafe && !contextualRisk,
    );
  }
  
  /// ğŸ§¹ ì‘ë‹µ ì •í™” (ì‹œìŠ¤í…œ ì •ë³´ ì œê±°ë§Œ)
  static String sanitizeResponse(String response) {
    // ì‹œìŠ¤í…œ ì •ë³´ ë³´í˜¸
    String cleaned = SystemInfoProtection.protectSystemInfo(response);
    
    // ë¯¼ê°í•œ ê¸°ìˆ  ìš©ì–´ ì œê±°
    final sensitiveTerms = [
      RegExp(r'gpt[-\d\.]*(turbo|mini|4|3\.5)?', caseSensitive: false),
      RegExp(r'openai|claude|anthropic', caseSensitive: false),
      RegExp(r'api\s*key|token|secret', caseSensitive: false),
      RegExp(r'firebase|flutter|dart', caseSensitive: false),
      RegExp(r'cloudflare|r2|cdn', caseSensitive: false),
      RegExp(r'prompt|instruction|system\s*message', caseSensitive: false),
    ];
    
    for (final pattern in sensitiveTerms) {
      cleaned = cleaned.replaceAll(pattern, '');
    }
    
    // ì‹œìŠ¤í…œ ê´€ë ¨ ë¬¸êµ¬ ì œê±°
    final systemPhrases = [
      'as an ai', 'as a language model', 'i am programmed',
      'my training', 'my model', 'ai assistant',
      'ì¸ê³µì§€ëŠ¥ìœ¼ë¡œì„œ', 'ì–¸ì–´ ëª¨ë¸ë¡œì„œ', 'í”„ë¡œê·¸ë˜ë°ëœ',
    ];
    
    for (final phrase in systemPhrases) {
      cleaned = cleaned.replaceAll(RegExp(phrase, caseSensitive: false), '');
    }
    
    // ë¹ˆ ë¬¸ì¥ ì •ë¦¬
    cleaned = cleaned
        .split('.')
        .where((s) => s.trim().isNotEmpty && s.trim().length > 2)
        .join('. ')
        .trim();
    
    return cleaned.isEmpty ? '' : cleaned;
  }
  
  /// ğŸ“Š ë¬¸ë§¥ ê¸°ë°˜ ìœ„í—˜ ë¶„ì„
  static bool _analyzeContextualRisk(
    String userMessage,
    List<String> recentMessages,
  ) {
    if (recentMessages.isEmpty) return false;
    
    // ë°˜ë³µì ì¸ ë¯¼ê°í•œ ì§ˆë¬¸ ê°ì§€
    int suspiciousCount = 0;
    for (final msg in recentMessages) {
      final detection = PatternDetectorService.detectPatterns(msg);
      if (detection.needsDeflection || detection.riskLevel > 0.5) {
        suspiciousCount++;
      }
    }
    
    // 3íšŒ ì´ìƒ ì—°ì† ë¯¼ê°í•œ ì§ˆë¬¸
    if (suspiciousCount >= 3) {
      debugPrint('âš ï¸ Repeated suspicious attempts detected');
      return true;
    }
    
    // ì ì§„ì  ê³µê²© íŒ¨í„´ ê°ì§€
    if (recentMessages.isNotEmpty) {
      final prevDetection = PatternDetectorService.detectPatterns(recentMessages.last);
      final currDetection = PatternDetectorService.detectPatterns(userMessage);
      
      // ìœ„í—˜ë„ ê¸‰ìƒìŠ¹
      if (currDetection.riskLevel > prevDetection.riskLevel && 
          currDetection.riskLevel - prevDetection.riskLevel > 0.3) {
        debugPrint('âš ï¸ Escalating risk pattern detected');
        return true;
      }
    }
    
    return false;
  }
  
  /// ğŸ¯ ì»¨í…ìŠ¤íŠ¸ íŒíŠ¸ ìƒì„± (OpenAI APIìš©)
  static String _buildContextHint(
    PatternDetectionResult detection,
    bool contextualRisk,
  ) {
    if (detection.isSafe && !contextualRisk) {
      return '';  // ì•ˆì „í•œ ëŒ€í™”ëŠ” íŒíŠ¸ ë¶ˆí•„ìš”
    }
    
    String hint = detection.contextHint;
    
    if (contextualRisk) {
      hint += '\n[ì£¼ì˜: ë°˜ë³µì ì¸ ë¯¼ê°í•œ ì§ˆë¬¸ íŒ¨í„´ ê°ì§€ë¨. ë”ìš± ìì—°ìŠ¤ëŸ½ê²Œ íšŒí”¼ í•„ìš”]';
    }
    
    if (detection.isHighRisk) {
      hint += '\n[ë†’ì€ ìœ„í—˜ë„: ë§¤ìš° ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ì£¼ì œ ì „í™˜ í•„ìš”]';
    }
    
    return hint;
  }
  
  /// âœ… ì‘ë‹µ ì•ˆì „ì„± ê²€ì¦
  static bool validateResponseSafety(String response) {
    final lowerResponse = response.toLowerCase();
    
    // ê¸ˆì§€ëœ í‚¤ì›Œë“œ ì²´í¬
    final forbiddenKeywords = [
      'api key', 'secret', 'token', 'password',
      'gpt-', 'claude', 'openai', 'anthropic',
      'firebase', 'flutter', 'cloudflare',
      'system prompt', 'initial prompt',
      'ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸', 'ì´ˆê¸° ì„¤ì •',
    ];
    
    for (final keyword in forbiddenKeywords) {
      if (lowerResponse.contains(keyword)) {
        debugPrint('âŒ Forbidden keyword detected: $keyword');
        return false;
      }
    }
    
    return true;
  }
  
  /// ğŸ“‹ ë³´ì•ˆ ì´ë²¤íŠ¸ ë¡œê¹…
  static void logSecurityEvent({
    required String eventType,
    required String userMessage,
    required SecurityAnalysisResult analysis,
  }) {
    if (kDebugMode) {
      debugPrint('ğŸ”’ Security Event: $eventType');
      debugPrint('Category: ${analysis.category}');
      debugPrint('Risk Level: ${analysis.riskLevel}');
      debugPrint('Needs Deflection: ${analysis.needsDeflection}');
      debugPrint('Message: ${userMessage.length > 50 ? userMessage.substring(0, 50) + "..." : userMessage}');
      debugPrint('Timestamp: ${DateTime.now().toIso8601String()}');
      
      if (analysis.isHighRisk) {
        debugPrint('âš ï¸ HIGH RISK DETECTED');
      }
    }
  }
}

/// ğŸ“Š ë³´ì•ˆ ë¶„ì„ ê²°ê³¼
class SecurityAnalysisResult {
  final bool needsDeflection;
  final String category;
  final double riskLevel;
  final String contextHint;
  final bool isHighRisk;
  final bool isSafe;
  
  const SecurityAnalysisResult({
    required this.needsDeflection,
    required this.category,
    required this.riskLevel,
    required this.contextHint,
    required this.isHighRisk,
    required this.isSafe,
  });
  
  /// OpenAI APIì— ì „ë‹¬í•  ë³´ì•ˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
  String toOpenAIContext() {
    if (isSafe) return '';
    
    final riskStr = isHighRisk ? 'ë†’ìŒ' : 
                    riskLevel > 0.5 ? 'ì¤‘ê°„' : 'ë‚®ìŒ';
    
    return '''
[ë³´ì•ˆ í•„í„° ê°ì§€]
ì¹´í…Œê³ ë¦¬: $category
ìœ„í—˜ë„: $riskStr
ëŒ€ì‘ ê°€ì´ë“œ: $contextHint
''';
  }
}